(window.webpackJsonp=window.webpackJsonp||[]).push([[69],{350:function(e,n,t){"use strict";t.r(n);var s=t(14),i=Object(s.a)({},(function(){var e=this,n=e._self._c;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("h1",{attrs:{id:"vqvae算法"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#vqvae算法"}},[e._v("#")]),e._v(" vqvae算法")]),e._v(" "),n("p",[e._v("VQ-VAE（Vector Quantised Variational AutoEncoder，矢量量化变分自动编码）是【1】提出的一种离散化VAE方案，近来【2】应用VQ-VAE得到了媲美于BigGan的生成模型。由此可见， VQ-VAE 有着强大的潜力，且【1】和【2】皆为DeepMind的作品，让我们通过代码来认识它，学习它。")]),e._v(" "),n("h2",{attrs:{id:"简介"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#简介"}},[e._v("#")]),e._v(" 简介")]),e._v(" "),n("p",[e._v("VQ-VAE属于VAE范畴，它有着与一般VAE都有的Encoder、code（编码）和Decoder，而不同之处在于其code并不是由Encoder直接输出得到，而是经过了一个矢量量化后才得到的，其结构图如下：\n"),n("img",{attrs:{src:"images/t2m/vqvae1.png",alt:""}}),e._v("\n图1 VQ-VAE结构图\n"),n("img",{attrs:{src:"images/t2m/vqvae2.png",alt:""}}),e._v("\n图2 VQ-VAE数据流图")]),e._v(" "),n("p",[e._v("结合 图1、图2 叙述其工作流程，下面基于采用了CIFAR10作为训练集情况下，各个数据结构描述。")]),e._v(" "),n("ol",[n("li",[e._v("输入x，其数据结构为[B,3,32,32]，，因此输入参数如此，B是batch的数量, 通道为3，图片长度宽度都是32像素；")]),e._v(" "),n("li",[e._v("经过Encoder，得到 $Z_e( x )$, 其结构为 [B, C=D, H, W]，其中C是指编码器的Conv网络输出的Channels 的数量，而D是指矢量量化中矢量的维度，也就是后续查表（Embedding）所存储矢量的维度，另外，H,W表示输入图像经编码器处理后的长和宽，本例中，编码器输入是32 * 32，输出时为8 * 8，即H=8, W=8；")]),e._v(" "),n("li",[e._v("将 $Z_e( x )$ 变形为 [B * H * W, D]，即每一个图片有 H*W 个编码，每个编码是D维，计算这些编码(B * H * W）与 Embedding 中 K 个矢量（在[3]中 K=512，表示矢量量化编码的矢量个数)之间的距离，通过最近邻算法构成如下映射：")])]),e._v(" "),n("p",[e._v("$$\nq(z=k|x) =\n\\begin{cases}\n1 \\ 如果k=argmin_j |Z_e(x) - e_j|^2\\\n0 \\ 其它\n\\end{cases} \\tag {1}\n$$")]),e._v(" "),n("p",[e._v("公式（1）表示当输入为 x 时，输出z = k 的概率是 ：\n1）当 k 是矢量序列 { $e_1, e_2, ⋯ , e_K$ } 中与 $Z_e(x)$最近的矢量的下标时，条件概率为1；\n2）否则为0。")]),e._v(" "),n("p",[e._v("这里的矢量距离度量采用常见的欧拉距离，公式（1）便是最近邻算法的实现。\n$$\nz_q(x) = e_k \\ 当k=argmin_i |Z_e(x) - e_j|^2 \\tag{2}\n$$")]),e._v(" "),n("p",[e._v("公式（2）表示的是，通过最近邻计算出与 $Z_e( x ) 最近的矢量的下标为 k ，然后查表将 $e_k$ 输出作为编码输出 $z_q( x )。")]),e._v(" "),n("ol",{attrs:{start:"4"}},[n("li",[e._v("$z_q(x)$ 作为Decoder的输入，有Decoder重建图像，输出 p(x∣z)")])]),e._v(" "),n("p",[e._v("由上分析，我们得知VQ-VAE的输出的每一个编码都是离散的，它们是保存在 Embedding 中 K个矢量中的某一个。在[3]的实验中，一幅图片在矢量量化后，将由8 * 8 个 64 维矢量表示，而这里的每个矢量都是 Embedding 中512个矢量中的一个。\n在整个实现中，有两个部分是很有特点的，其一就是上面讲的矢量量化过程，另外一个就是Loss的计算。在[1]中，Loss分为三个部分，如下：\n$$\nLoss = log p(x|z_q(x)) + |sg[Z_e(x)] - e|_2^2 + \\beta|Z_e(x) - sg[e]|_2^2\n$$\n第一项 $log⁡ p(x∣z_q(x))$ 表示重构误差，这是二进制交叉熵的形式；第二项是用于update 在 Embedding中的字典项的Loss，其中 $sg[\\cdot]$ 表示 stop gradient，即不执行后向梯度传递，因此，该项只对字典项（矢量量化中矢量）学习有效；第三项是对Encoder有效的Loss，其解释如下：")]),e._v(" "),n("p",[e._v("由于 嵌入空间(the embedding space) 中的量是无量纲的，如果 $e_i$ 的训练速度跟不上encoder参数的训练速度的话，它就可能增大到任意值。因此，为使encoder能给出一个合理的embedding，于是就给encoder加上了一个惩罚项，其中 β  可选为 [ 0.1 , 2 ]，本例中选为β = 0.25 。")]),e._v(" "),n("p",[e._v("我的理解是，如果encoder与embedding之间若无一个约束的话，则encoder的输出会严重偏离embedding中的矢量，")]),e._v(" "),n("p",[e._v("另外，在backward时，重构误差梯度信息是直接传给Encoder的，但实际上Encoder的信息并不是直接被Decoder使用的，中间有Embedding转换一道，这是合理的吗？文章【1】给出的解释如下：")]),e._v(" "),n("p",[e._v("这就是图2中红线标注出来的地方 $(∇_z L)$，因为Embedding最终输出的矢量维度与Encoder输出矢量的维度相同，而且相似，因此可认为梯度 $(∇_z L)$ 也可以改善Encoder的重构误差。这种说法虽然不严谨，但从实验结果上看，是行得通的。\n小结一下Loss的作用对象：")]),e._v(" "),n("ol",[n("li",[e._v("Encoder 受 $log⁡ p(x∣z_q(x))  和 β ∥ Z_e(x) − sg[e]∥^2_2$  影响；")]),e._v(" "),n("li",[e._v("Embedding 受 $log⁡ p(x∣z_q(x)) 和 ∥ sg[ Z_e(x) ] − e∥^2_2$ 影响；")]),e._v(" "),n("li",[e._v("Decoder 受 $log p(x∣z_q(x))$。")])]),e._v(" "),n("h2",{attrs:{id:"代码实现"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#代码实现"}},[e._v("#")]),e._v(" 代码实现")]),e._v(" "),n("p",[e._v("完整的代码在[3]中，我们在这里就不一一详述了，只对一些有点特点的部分写一些注释。\n我们先来看看 Embedding的实现：")]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("class VectorQuantizer(nn.Module):\n    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n        super(VectorQuantizer, self).__init__()\n        \n        self._embedding_dim = embedding_dim\n        self._num_embeddings = num_embeddings\n        \n        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n        self._commitment_cost = commitment_cost\n\n    def forward(self, inputs):\n        # convert inputs from BCHW -> BHWC\n        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n        input_shape = inputs.shape\n        \n        # Flatten input\n        flat_input = inputs.view(-1, self._embedding_dim)\n        \n        # Calculate distances\n        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n                    + torch.sum(self._embedding.weight**2, dim=1)\n                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n            \n        # Encoding\n        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings).to(device)\n        encodings.scatter_(1, encoding_indices, 1)\n        \n        # Quantize and unflatten\n        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n        \n        # Loss\n        e_latent_loss = torch.mean((quantized.detach() - inputs)**2)\n        q_latent_loss = torch.mean((quantized - inputs.detach())**2)\n        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n        \n        quantized = inputs + (quantized - inputs).detach()\n        avg_probs = torch.mean(encodings, dim=0)\n        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n        \n        # convert quantized from BHWC -> BCHW\n        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings\n")])])]),n("p",[e._v("以下将就上述代码进行解读：")]),e._v(" "),n("ol",[n("li",[e._v("此处，Embedding通过 nn.Embedding 来实现，其中有 num_embeddings=512，embedding_dim = 64。对它的初始化，仅仅是通过uniform随机分布来设置初值。")]),e._v(" "),n("li",[e._v("Encoder的输出是[ B * C * H * W ]，意味着一幅图片经 Encoder 提取的特征值是 [ C * H * W ]，将其整型为 [ H * W， C ] 即一幅图片有 H * W 个特征，每个特征的编码是 C 维。")]),e._v(" "),n("li",[e._v("计算距离：")])]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("distances = (torch.sum(flat_input**2, dim=1, keepdim=True) + torch.sum(self._embedding.weight**2, dim=1) - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n\n")])])]),n("p",[e._v("其实 torch.sum(flat_input2, dim=1, keepdim=True) 的维度与 torch.sum(self._embedding.weight2, dim=1) 的维度是不同的，前者是2048 * 1，而后者是 512*1，这样加的结果是 2048 * 512，即前者的每一个元素与后者的每一个元素相加得到的结果，得到一个2048 * 512矩阵，恰好与后面的 torch.matmul(flat_input, self._embedding.weight.t()) 维度相同。这里计算的是：\n$$\n|Z_e(x) - e_j|^2\n$$")]),e._v(" "),n("p",[e._v("接下来：")]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("# Encoding\nencoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\nencodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings).to(device)\nencodings.scatter_(1, encoding_indices, 1)\n\n")])])]),n("p",[e._v("这里是计算")]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("k = argmin_j|Z_e(x) -e_j|^2\n")])])]),n("p",[e._v("由torch.argmin(distances, dim=1)获得每一个特征矢量对应的embedding矢量的索引值 k，再由它作为encoding_indices 将 1 分配到形如：[ B * H * W, K] 的矩阵上，其中 K 是embedding 字典中矢量的个数（即 num_embeddings） 。此 k 的形式（即 encodings 的形式）如下：\n$$\nq(z|x) =\n\\left[\n\\begin{matrix}\n0 & 0 & 1 &\\cdots & 0\\\n0 & 1 & 0 &\\cdots & 0 \\\n\\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\n0 & 1 & 0 & \\cdots & 0\n\\end{matrix}\n\\right]")]),e._v(" "),n("p",[e._v("$$\nEncodingIndices 矩阵共 B * H * W 行，每行有一个 1 其余皆为0， 矩阵共有 K 列，同1列上可以有多个1。\n得到EncodingIndices 矩阵后，只需与Embedding矩阵相乘，便可以实现矢量量化的输出，如下：\n$$\nquantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n$$")]),e._v(" "),n("p",[e._v("矩阵乘法 [B * H * W, K] * [ K, D] = [ B * H * W, D]，即：\n$$\nz_q(x) =\n\\left[\n\\begin{matrix}\n0 & 0 & 1 &\\cdots & 0\\\n0 & 1 & 0 &\\cdots & 0 \\\n\\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\n0 & 1 & 0 & \\cdots & 0\n\\end{matrix}\n\\right]")]),e._v(" "),n("ul",[n("li",[e._v("[e_1\\ e_2\\ \\cdots e_K]^T \\\n=\n\\left[\n\\begin{matrix}\ne_3\\\ne_2\\\n\\vdots\\\ne_2\n\\end{matrix}\n\\right]")])]),e._v(" "),n("p",[e._v("$$")]),e._v(" "),n("p",[e._v("接下来是计算与 Embedding 相关的 Loss 计算：")]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[e._v("# Loss\ne_latent_loss = torch.mean((quantized.detach() - inputs)**2)\nq_latent_loss = torch.mean((quantized - inputs.detach())**2)\nloss = q_latent_loss + self._commitment_cost * e_latent_loss\n\n")])])]),n("p",[e._v("这部分代码对应：\n$$\n|sg[Z_e(x)] - e|_2^2 + \\beta|Z_e(x) - sg[e]|_2^2\n$$\n上式的 $sg[\\cdot]$ 表示停止梯度（stop gradient），在实现时，我们看到用了Tensor.detach() 来实现，语法理解真是很精准，实现得很简练，这是佩服作者对Pytorch掌握的熟练，什么时候我才能也达到这个高度呢？")]),e._v(" "),n("p",[e._v("至于其他部分的代码，还有值得学习的，但我在这里就不多说了，有兴趣的同学可以看看[3]，原滋原味。")]),e._v(" "),n("h2",{attrs:{id:"小结"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#小结"}},[e._v("#")]),e._v(" 小结：")]),e._v(" "),n("p",[e._v("VQ-VAE 通过离散的矢量对code进行量化和编码，压缩了编码空间，却达到可以媲美连续编码空间的重构效果，为图像的矢量化提供了一种可能的方法。")]),e._v(" "),n("h2",{attrs:{id:"参考"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#参考"}},[e._v("#")]),e._v(" 参考")]),e._v(" "),n("p",[e._v("[1] Neural Discrete Representation Learning, arXiv:1711.00937v2 [cs.LG] 30 May 2018\n[2] Generating Diverse High-Fidelity Images with VQ-VAE-2，arXiv:1906.00446v1 [cs.LG] 2 Jun 2019\n[3] https://github.com/zalandoresearch/pytorch-vq-vae\n————————————————")]),e._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",[n("code",[e._v("                        版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。\n")])])]),n("p",[e._v("原文链接：https://blog.csdn.net/StreamRock/article/details/93881187")])])}),[],!1,null,null,null);n.default=i.exports}}]);