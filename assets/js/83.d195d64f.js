(window.webpackJsonp=window.webpackJsonp||[]).push([[83],{364:function(t,e,r){"use strict";r.r(e);var a=r(14),n=Object(a.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"tvm是什么"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#tvm是什么"}},[t._v("#")]),t._v(" TVM是什么？")]),t._v(" "),e("p",[t._v("Apache TVM（Tensor Virtual Machine）是一个开源机器学习编译器框架，适用于 CPU、GPU 和机器学习加速器。它旨在任何硬件后端上优化并高效运行计算。")]),t._v(" "),e("h1",{attrs:{id:"深度学习应用简介"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#深度学习应用简介"}},[t._v("#")]),t._v(" 深度学习应用简介")]),t._v(" "),e("p",[t._v("深度学习应用分为两个过程：")]),t._v(" "),e("ul",[e("li",[t._v("Training 训练：从数据得到模型；")]),t._v(" "),e("li",[t._v("Inference 推理：从模型得到答案。\n为解决训练而设计的系统叫训练框架：比如 paddlepaddle，tensorflow，pytorch；\n为解决推理而设计的系统叫推理引擎：比如 paddle inference，tensorflow，pytorch-trt，TensorRT 等等。\n训练得到的模型，其实就是一个计算图，这个计算图接收输入，通过一系列的运算，得到一个结果，后面这个过程就叫推理。")])]),t._v(" "),e("h1",{attrs:{id:"深度学习工程化中的问题"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#深度学习工程化中的问题"}},[t._v("#")]),t._v(" 深度学习工程化中的问题")]),t._v(" "),e("p",[t._v("还是从上面的两个过程来分析，模型基本只需要Training一次，但是模型会在不同的设备上去推理。在工程化的过程中就需要解决：")]),t._v(" "),e("ul",[e("li",[t._v("推理的性能问题；")]),t._v(" "),e("li",[t._v("硬件的支持问题。\n为了在多种多样的设备中都保持一个高效的推理性能，各大硬件厂商都推出了自己的推理框架，比如Intel的OpenVINO，ARM的ARM NN，NV的TensorRT等。但是这里面有一个问题，各大设备厂商的框架并不具备通用性，通常在一个设备厂商的Inference框架能跑，但是不一定在另外一个设备厂商的Inference框架上能跑。\n"),e("img",{attrs:{src:"/images/tvm/principle.jpg",alt:""}})])]),t._v(" "),e("h1",{attrs:{id:"引入编译器思想"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#引入编译器思想"}},[t._v("#")]),t._v(" 引入编译器思想")]),t._v(" "),e("p",[t._v("我们曾经出现了很多种编程语言，有很多种硬件，历史上最开始也是一种语言对应一种硬件，从而造成编译器的维护困难与爆炸。而编译器后面解决了这个问题，其具体解决办法是这样的：抽象出编译器前端，编译器中端，编译器后端等概念，引入IR (Intermediate Representation)。")]),t._v(" "),e("ul",[e("li",[t._v("编译器前端：接收C / C++ / Fortran等不同语言，进行代码生成，吐出IR")]),t._v(" "),e("li",[t._v("编译器中端：接收IR，进行不同编译器后端可以共享的优化，如常量替换，死代码消除，循环优化等，吐出优化后的IR")]),t._v(" "),e("li",[t._v("编译器后端：接收优化后的IR，进行不同硬件的平台相关优化与硬件指令生成，吐出目标文件\n"),e("img",{attrs:{src:"/images/tvm/wf0.jpg",alt:""}}),t._v("\n其实对于Inference框架也是类似的道理，我们也想要得到类似的架构：\n"),e("img",{attrs:{src:"/images/tvm/workflow.jpg",alt:""}})])]),t._v(" "),e("h1",{attrs:{id:"tvm做了哪些工作"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#tvm做了哪些工作"}},[t._v("#")]),t._v(" TVM做了哪些工作")]),t._v(" "),e("p",[t._v("从官网的优化步骤图可以看到：\n"),e("img",{attrs:{src:"/images/tvm/wf1.jpg",alt:""}})]),t._v(" "),e("ul",[e("li",[t._v("步骤1，即前端：从 TensorFlow、PyTorch 或 ONNX 等框架导入模型；")]),t._v(" "),e("li",[t._v("步骤2——6，即中端：导入的模型翻译成Relay IR，完成各种图结构的优化如死代码删除，算符融合；转换成 TE（Tensor Expression）然后经过一系列的优化、降级，最终得到TIR；")]),t._v(" "),e("li",[t._v("步骤7，即后端：编译成目标机器码，支持LLVM、CUDA、OpenCL等。")])])])}),[],!1,null,null,null);e.default=n.exports}}]);